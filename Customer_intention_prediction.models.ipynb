{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "The goal is to identify which features are important and influence the buying intent of customers. There are 3 sets of feature and 2 models are trained on each set. The tree based model(Gradient boosted trees) will be used to determine feature importance. The model with the highest Balanced accuracy score will be selected. The neural netwrok based mode serves as reference for how good the tree based model is. Ideally the performance between the two models should be equal. \n",
    "\n",
    "\n",
    "## Model selection\n",
    "\n",
    "The hyper parameter for each model is optimized using bayesian optimization and 5 fold  stratified Cross Validation. Each model is trained on there different transformations of the original dataset. Since the data set is imbalanced be use balanced accuracy as the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization and selection for gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from lightgbm) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn->lightgbm) (0.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (2.2)\n",
      "Requirement already satisfied: six in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from hyperopt) (1.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from hyperopt) (1.18.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (4.28.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (0.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\swati\\appdata\\roaming\\python\\python37\\site-packages (from hyperopt) (1.4.1)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt) (0.17.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from networkx>=2.2->hyperopt) (4.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import lightgbm as lgb #light gradient boosted tree\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold # train and test split\n",
    "from sklearn.metrics import balanced_accuracy_score,precision_score# metrics\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK# optimization\n",
    "import numpy as np\n",
    "import pandas as pd # reading data\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_encoded = pd.read_csv('online_shoppers_intention_encoded.csv', index_col=False)\n",
    "df_scaled = pd.read_csv('online_shoppers_intention_encoded_scaled.csv', index_col=False)\n",
    "df_transformed = pd.read_csv('online_shoppers_intention_encoded_scaled_transformed.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate label\n",
    "label = df_encoded.pop('Revenue').astype('int')\n",
    "_,_ = df_scaled.pop('Revenue'),df_transformed.pop('Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split object for CV \n",
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "#Hyperparameter search space for gradient boosed trees\n",
    "lgb_space = {'lr':hp.loguniform('lr',-6.9,-2.3),'num_leaves':hp.quniform('num_leaves',15,255,1),\n",
    "         'max_depth':hp.choice('max_depth',[-1,9,12]),'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_optimizer(params):\n",
    "    '''A function to optimize  lgb classifier\n",
    "    :params:=params , dictionary containing the Hyper-parameters for the classifier\n",
    "    returns true loss and validation loss\n",
    "    '''\n",
    "    if 'num_leaves' in params:\n",
    "        params['num_leaves']=int(params['num_leaves'])\n",
    "    if 'max_depth' in params:\n",
    "        params['max_depth']=int(params['max_depth'])\n",
    "    val_score=[]\n",
    "    true_scores=[]\n",
    "    rd=1\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        clf = lgb.LGBMClassifier(n_estimators=2000,**params)\n",
    "        clf.fit(X_tr,y_tr,eval_set=(X_val,y_val),early_stopping_rounds =200,eval_metric='logloss',verbose=False)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        y_tr_pred=clf.predict(X_tr)\n",
    "        score=balanced_accuracy_score(y_val,y_pred,)\n",
    "        true_score=balanced_accuracy_score(y_tr,y_tr_pred)\n",
    "        val_score.append(score)\n",
    "        true_scores.append(true_score)\n",
    "        rd+=1\n",
    "    mean,std =np.mean(val_score),np.std(val_score)\n",
    "    true_mean=np.mean(true_scores)\n",
    "    print(\"mean: {}, Std: {}\".format(mean,std))\n",
    "    return {'loss':-mean,'status': STATUS_OK,'true_loss':-true_mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.779801200445387, Std: 0.015378072232663909                                                                     \n",
      "mean: 0.7737058846317746, Std: 0.013829952496060574                                                                    \n",
      "mean: 0.7739169123334954, Std: 0.013047148989631644                                                                    \n",
      "mean: 0.7443563883164647, Std: 0.01709156664462584                                                                     \n",
      "mean: 0.770928183223507, Std: 0.010748851779302045                                                                     \n",
      "mean: 0.7695350117407708, Std: 0.01202891358575693                                                                     \n",
      "mean: 0.775500404272416, Std: 0.017958117750630293                                                                     \n",
      "mean: 0.776527349934954, Std: 0.012509161521363357                                                                     \n",
      "mean: 0.7731379287505795, Std: 0.006790062729263657                                                                    \n",
      "mean: 0.7629204276914733, Std: 0.010534321964305041                                                                    \n",
      "mean: 0.7748911401231938, Std: 0.012986189117226522                                                                    \n",
      "mean: 0.7721109830880414, Std: 0.012754724607621261                                                                    \n",
      "mean: 0.760765355790968, Std: 0.004285153780096473                                                                     \n",
      "mean: 0.7742563512008829, Std: 0.018319934922376952                                                                    \n",
      "mean: 0.7620006235917636, Std: 0.012939987258316705                                                                    \n",
      "mean: 0.7723691738221425, Std: 0.010349190831989917                                                                    \n",
      "mean: 0.7828762201591346, Std: 0.012744083682648072                                                                    \n",
      "mean: 0.7708209229408037, Std: 0.00587495460714571                                                                     \n",
      "mean: 0.7744847245011578, Std: 0.013388807830565373                                                                    \n",
      "mean: 0.7580124043523956, Std: 0.0053172993093185275                                                                   \n",
      "100%|███████████████████████████████████████████████| 20/20 [02:49<00:00, 11.12s/trial, best loss: -0.7828762201591346]\n"
     ]
    }
   ],
   "source": [
    "#split into train and test and call the fmin(optimizer) function for encoded dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_encoded,label,test_size=0.2,random_state=42)\n",
    "trials_encoded = Trials()\n",
    "best_e= fmin(lgb_optimizer,lgb_space, algo=tpe.suggest, max_evals=20, trials=trials_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.7692049080045937, Std: 0.011876834499231817                                                                    \n",
      "mean: 0.7653190502909887, Std: 0.020089213375784665                                                                    \n",
      "mean: 0.7772322273223169, Std: 0.013845146511467876                                                                    \n",
      "mean: 0.788156955172625, Std: 0.013054956347022348                                                                     \n",
      "mean: 0.7527307758156367, Std: 0.008792092461853668                                                                    \n",
      "mean: 0.7658385434695946, Std: 0.005584254936410054                                                                    \n",
      "mean: 0.7775213914469203, Std: 0.011446310628948594                                                                    \n",
      "mean: 0.7598946454837086, Std: 0.01574105291905567                                                                     \n",
      "mean: 0.7294897107359007, Std: 0.012443175838917823                                                                    \n",
      "mean: 0.7732286307349511, Std: 0.016314530135782913                                                                    \n",
      "mean: 0.7689569459249715, Std: 0.010098665511287772                                                                    \n",
      "mean: 0.7711117553920692, Std: 0.008152233015738548                                                                    \n",
      "mean: 0.765992704351271, Std: 0.023948357693204192                                                                     \n",
      "mean: 0.7584940696298312, Std: 0.014078181114404065                                                                    \n",
      "mean: 0.7673194364430026, Std: 0.0066175911269464064                                                                   \n",
      "mean: 0.7646998199206951, Std: 0.0184818448290758                                                                      \n",
      "mean: 0.7673667181952579, Std: 0.010090473899328357                                                                    \n",
      "mean: 0.7795611676037206, Std: 0.020015056599218657                                                                    \n",
      "mean: 0.7535816724006283, Std: 0.01750371483172375                                                                     \n",
      "mean: 0.7767272554589273, Std: 0.010728262351345257                                                                    \n",
      "100%|████████████████████████████████████████████████| 20/20 [03:43<00:00,  9.88s/trial, best loss: -0.788156955172625]\n"
     ]
    }
   ],
   "source": [
    "#for scaled dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_scaled,label,test_size=0.2,random_state=42)\n",
    "trials_scaled = Trials()\n",
    "best_scaled = fmin(lgb_optimizer, lgb_space, algo=tpe.suggest, max_evals=20, trials=trials_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.7559017899208076, Std: 0.018882095745168123                                                                    \n",
      "mean: 0.7668276612309626, Std: 0.011003723453735226                                                                    \n",
      "mean: 0.7795804689560024, Std: 0.008331011736322908                                                                    \n",
      "mean: 0.770613900472755, Std: 0.009989144471381222                                                                     \n",
      "mean: 0.7710507396248202, Std: 0.012526496267674297                                                                    \n",
      "mean: 0.7717355408565076, Std: 0.022578656628019948                                                                    \n",
      "mean: 0.7575867123717356, Std: 0.01666954514540512                                                                     \n",
      "mean: 0.7672287469554601, Std: 0.015779766422753253                                                                    \n",
      "mean: 0.7787505795404416, Std: 0.012623762253726903                                                                    \n",
      " 45%|█████████████████████▌                          | 9/20 [01:30<02:02, 11.12s/trial, best loss: -0.7795804689560024]"
     ]
    }
   ],
   "source": [
    "#for transformed dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_transformed,label,test_size=0.2,random_state=42)\n",
    "trials_transformed = Trials()\n",
    "best_transformed = fmin(lgb_optimizer, lgb_space, algo=tpe.suggest, max_evals=20, trials=trials_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(Data,label,params,classifier='lgb',test_size=0.2):\n",
    "    '''A function to train a model. \n",
    "    parameters:\n",
    "    Data :=pandas dataset or numpy array of features\n",
    "    label:=pandas dataset or numpy array of labels for features \n",
    "    params:=python dict parametes for the model\n",
    "    model:=keras model object or sklearn classifier object\n",
    "    test_size:=Float (0,1) fraction for test split\n",
    "    return\n",
    "    model:=str 'keras','lgb'\n",
    "    train_score:=float balanced accuracy train score\n",
    "    test_score:=float balanced accuracy test score\n",
    "    returns-trained model,train_score,test_score\n",
    "    '''\n",
    "    X_train,X_test,y_train,y_test = train_test_split(Data,label,test_size=test_size,random_state=42)\n",
    "    if classifier=='lgb':\n",
    "        if 'num_leaves' in params:\n",
    "            params['num_leaves']=int(params['num_leaves'])\n",
    "        if 'max_depth' in params:\n",
    "            #max_depth=[-1,9,12]#max_depth choice list\n",
    "            params['max_depth']=-1#int(params['max_depth'])#best returns an index corresponding the choice\n",
    "        clf = lgb.LGBMClassifier(n_estimators=5000,**params)\n",
    "        clf.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds =200,eval_metric='auc',verbose=False)\n",
    "        train_pred=clf.predict(X_train)\n",
    "        test_pred=clf.predict(X_test)\n",
    "    elif classifier=='keras':\n",
    "        opt=['adam','sgd']\n",
    "        if 'optimizer' in params:\n",
    "            params['optimizer'] =opt[params['optimizer']]\n",
    "        clf = model(feature_size=X_train.shape[-1],**params)\n",
    "        clf.fit(X_train,y_train,epochs=20,batch_size=128,verbose=0)\n",
    "        train_pred=np.round(clf.predict(X_train))\n",
    "        test_pred=np.round(clf.predict(X_test))\n",
    "    train_score = balanced_accuracy_score(train_pred,y_train)\n",
    "    test_score= balanced_accuracy_score(test_pred,y_test)\n",
    "    return clf,train_score,test_score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_e)\n",
    "print(best_scaled)\n",
    "print(best_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_encoded,lgb_train_encoded,lgb_test_encoded=model_trainer(df_encoded,label,best_e)\n",
    "clf_scaled,lgb_train_scaled,lgb_test_scaled=model_trainer(df_scaled,label,best_scaled)\n",
    "clf_transformed,lgb_train_transformed,lgb_test_transformed=model_trainer(df_transformed,label,best_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results\")\n",
    "print(\"| Dataset Type | Train Score | Test Score |\")\n",
    "print(\"| Encoded      | {:2f}       | {:2f}     |\".format(lgb_train_encoded,lgb_test_encoded))\n",
    "print(\"| Scaled       | {:2f}       | {:2f}     |\".format(lgb_train_scaled,lgb_test_scaled))\n",
    "print(\"| Transformed  | {:2f}       | {:2f}     |\".format(lgb_train_scaled,lgb_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimization and selection for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(feature_size,lr=0.1,optimizer='adam',hl_size=128):\n",
    "    ''' function to define keras model\n",
    "    :feature:=\n",
    "    '''\n",
    "    if optimizer=='adam':\n",
    "        opt =Adam(lr=lr)\n",
    "    elif optimizer=='sgd':\n",
    "        opt = SGD(lr=lr)\n",
    "    hl_size=int(hl_size)\n",
    "    model =Sequential()\n",
    "    model.add(Dense(hl_size,activation='relu',input_shape=(feature_size,)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(hl_size,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['acc'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_optimizer(params):\n",
    "    val_score=[]\n",
    "    true_scores=[]\n",
    "    rd=1\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        nn_model = model(X_train.shape[1],**params)\n",
    "        cb = EarlyStopping(monitor='val_acc',min_delta=0.001,patience=3)\n",
    "        nn_model.fit(X_tr,y_tr,validation_data=(X_val,y_val),batch_size=128,callbacks=[cb],epochs=50,verbose=0)\n",
    "        #_,score= nn_model.evaluate(X_val,y_val)\n",
    "        #_,true_score=nn_model.evaluate(X_tr,y_tr)\n",
    "        cv_pred = np.round(nn_model.predict(X_val))\n",
    "        score=balanced_accuracy_score(y_val,cv_pred)\n",
    "        y_tr_pred = np.round(nn_model.predict(X_tr))\n",
    "        true_score=balanced_accuracy_score(y_tr,y_tr_pred)\n",
    "        \n",
    "        val_score.append(score)\n",
    "        true_scores.append(true_score)\n",
    "        rd+=1\n",
    "    mean,std =np.mean(val_score),np.std(val_score)\n",
    "    true_mean=np.mean(true_scores)\n",
    "    print(\"mean: {}, Std: {}\".format(mean,std))\n",
    "    return {'loss':-mean,'status': STATUS_OK,'true_loss':-true_mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_space = {'lr':hp.loguniform('lr',-10,-2.3),'optimizer':hp.choice('optimizer',['adam','sgd'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_encoded,label,test_size=0.2,random_state=42)\n",
    "best_nn_encoded=fmin(keras_optimizer,keras_space,algo=tpe.suggest,max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_scaled,label,test_size=0.2,random_state=42)\n",
    "best_nn_scaled=fmin(keras_optimizer,keras_space,algo=tpe.suggest,max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_transformed,label,test_size=0.2,random_state=42)\n",
    "best_nn_transformed=fmin(keras_optimizer,keras_space,algo=tpe.suggest,max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_nn_encoded)\n",
    "print(best_nn_scaled)\n",
    "print(best_nn_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,nn_train_encoded,nn_test_encoded=model_trainer(df_encoded,label,best_nn_encoded,'keras')\n",
    "_,nn_train_scaled,nn_test_scaled=model_trainer(df_scaled,label,best_nn_scaled,'keras')\n",
    "_,nn_train_transformed,nn_test_transformed=model_trainer(df_transformed,label,best_nn_transformed,'keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"|              |        neural network            |       Boosted Trees       |\")\n",
    "print(\"| Dataset Type | Train Score     | Test Score     |Train Score   | Test Score |\")\n",
    "print(\"| Encoded      | {:.4f}          | {:.4f}         | {:.4f}       | {:.4f}     |\".format(nn_train_encoded,nn_test_encoded,lgb_train_encoded,lgb_test_encoded))\n",
    "print(\"| Scaled       | {:.4f}          | {:.4f}         | {:.4f}       | {:.4f}     |\".format(nn_train_scaled,nn_test_scaled,lgb_train_scaled,lgb_test_scaled))\n",
    "print(\"| Transformed  | {:.4f}          | {:.4f}         | {:.4f}       | {:.4f}     |\".format(nn_train_scaled,nn_test_scaled,lgb_train_scaled,lgb_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for both tree based model and neural network model are obtained on the feature set that is not scaled or transfromed. Both models perform equally well with the neural netwrok performing 1 percent better than the gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf_encoded, 'lgb_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance for the best tree based model\n",
    "%matplotlib inline\n",
    "lgb.plot_importance(clf_encoded,max_num_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The top 10 features affecting the buying intentions were identified. These features can be further isolated depending upon the applications. For eg. Exit would be a good measure of how well personalized webpages are working for users. A simple A/B test can be carried out with and without personalization and exit rates as well as other features can be monitored. Change in these features would indicate a change in buying intent and tell us if the test was succesfull. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
